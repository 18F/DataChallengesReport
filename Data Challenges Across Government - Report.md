## ![image alt text](image_0.png) 

## Data challenges across government

### In spring of 2016, 18F interviewed 31 agency leaders and data professionals across 20 different agencies, and asked them what their biggest pain points were concerning using data to further their agency’s mission. Despite the wide breadth of uses data serves across those agencies, we found astounding agreement in what the biggest challenge is: **18 of the 20 agencies we spoke with cited data sharing as a primary barrier to using data to further their mission**. Specific reasons for data sharing pain is policy or culture (11 agencies), the technical mechanics of sharing data (8 agencies), and data discovery (6 agencies). Other common pain points we found include data integrity (9 agencies), lack of staff or specific skills (8 agencies), and IT policy (5 agencies).

*18F would like to thank participants for their thoughts and time. For questions, concerns, or further discussion, please reach out to **[anthony.garvan@gsa.go*v](mailto:anthony.garvan@gsa.gov)*. *

# Method

We heard from **31** individuals in **22** separate interviews with **20 **different agencies. Most interviews were 45 minutes, a handful were 30 minutes, and one response was sent via email.

We asked our interviewees the questions below, in this order, and  focused on digging into the first two questions. 

1. What are the biggest pain points for your organization concerning data? 

2. What are your "data dreams," or things that you hope to do with your data someday that you cannot achieve today?

3. What gaps do you have in your capabilities to work with data effectively?

4. Would you be willing to work through IAAs with TTS to get those needs met? What factors would make an IAA a viable solution (cost, duration, etc.)?

5. Who else in your agency would be good for us to talk to for the purposes of this research?

6. Do you know anyone who has a strong need for data work, such as reporting, visualization, analysis, or storage, or consulting, and if so could you give us their contact info?

7. Do you have data that is of value to the public, or you’d want to be open, that you need help making available? If so, what are the blockers to doing so? For example, Do you have any scalability challenges with data hosting, e.g. large volumes of data or high demand internally or externally?

8. Is there data from other sources that would be helpful to integrate with your agency data, in order to help fulfill your agency mission?

9. Do you have sufficient dedicated data analysts, statisticians, or data scientists on staff/contractors? (Not DBAs or excel-level analysts) Would those roles be helpful at your agency? 

10. Is there an understanding of the value of open data in your agency’s culture?

11. What are your thoughts on building vs. buying data solutions?

12. Budget for data? Have things you’ve wanted to pursue in data been cost prohibitive in the past? If so, could you tell us what they were?

13. Are you aware of data.gov? Digital Analytics Platform? Api.data.gov?

14. What core agency activities would "additional data" (visualization/processing/statistics) make more effective?

We always had one researcher focus on discussion and  the other on note taking. We took over 65 pages of notes. We’re keeping those notes private out of respect for our participants. 

We spoke with:

Consumer Financial Protection Bureau

Department of Defense

Department of Justice

Department of Labor

Department of State

Department of Transportation

Department of Treasury

Environmental Protection Agency

Federal Communications Commission

Federal Election Commission

Federal Emergency Management Agency

General Services Administration

Internal Revenue Service

Millennium Corporate Challenge

U.S. Agency for International Development

U.S. Air Force

U.S. Department of Agriculture

U.S. Patent and Trademark Office

United States Postal Service Office of Inspector General

White House Office of Science and Technology Policy

Our participants fell into three broad job categories.

**Role						#**

CIO / CTO / CDO….………..……………..11

Manager / Advisor / Director…..………....10		

Analyst / Developer..……....….…………..10

Results

Here are the number agencies that reported these challenges.

**Challenge reported	   # Agencies reported**

Data sharing…………...…….18

Data integrity……………........9

Staff / skills…………………....8

IT policy……………...………..5

Data sharing is a major challenge. Here are specific parts of the data sharing problem that various agencies reported.

**Data sharing challenge      # Agencies reported**

Policy / culture..…………….11

Technical..…………….……...8

Discovery / inventory………..6

Discussion

Data is used for a tremendous variety of purposes for the agencies we spoke with — from promoting transparency in elections, to tracking over a trillion dollars in spending, to modelling changes in mortgage prices, to fighting ISIS. Given the variety of datasets the government generates and uses, you might expect a wide variety of problems. It was surprising to us that such a clear pattern emerged from our research: data sharing is very consistently cited as a leading source of frustration among data professionals across nearly all the agencies and roles we spoke with. The problem is specific to data that is neither public nor classified. There are clear policies around classified data (which is far rarer anyway), and public data publishing and discovery is handled by data.gov, which is a widely known and widely used solution, and a variety of agency data portals. 

Participants cited difficulty in sharing data due to policy or culture, due to technical reasons, and because it was difficult to discover what data exists. 

From the policy perspective: although in some cases data is restricted by statutory law (tax data, for example), in most cases the participants had full legal rights to the data, but still couldn’t access it because data owners cite nebulous or restrictive policies to refuse access. Government is risk-averse, and if there is not a clear policy stating that you *can and should *share a particular type of data with a particular group, data owners default to not sharing. Sometimes the policy *is *clear, but the process of getting approvals is too cumbersome. In other cases, more detailed policy is required to provide clarity around protocols that streamline sharing: for example, when and how to broadcast changes in data formats, or how to describe data sets effectively.

Closely related to data policy is the lack of a data-sharing culture. A lot of data is not restricted by law, or even by policy, but data owners refuse to share, or drag their feet, because incentives just don’t encourage them to share. Besides sometimes taking work to clean up and prepare, giving up the data is a forfeiture of some level of control the individual has, and may expose them or their departments to risk if the data is erroneous, or the data itself may be used against them by comparing it with that of other departments. These disincentives to share are strong, and may even outweigh formal policy in some cases. We heard from one participant who claimed that getting the data they need to do their job was "like pulling teeth" but that clearer policy was unlikely to help. Another cited sharing as a primary challenge, but estimated the problem was “only 10% technical,” since winning over the hearts and minds of the data owners could quickly change the technical landscape. For data owners to start finding reasons to share, rather than reasons to not to, they’ll need to see the benefits of sharing data outweigh their personal effort and risk*.*

Others spoke about the data sharing problem from the technical perspective. We heard from several people, for example, that siloing made it difficult to share data. Several participants mentioned that legacy systems built up over time tended to be incompatible,, which leads to a lot of work to get a "big picture," if it’s even possible. In some cases, the pace of technology poses somewhat unique challenges for long-running government projects. The Department of Transportation, for example, might study a single segment of highway for 30 years. Over that time, technology and science evolved tremendously, so it makes sense to measure new things in new ways, yet that very progress makes it difficult to deliver continuity in the dataset overall. Often when we hear the word “legacy system,” we think of technologies that have fallen out of fashion, yet in some cases it’s the data itself, and not the technologies around it, that’s from a different era. 

We also heard from agencies who said that just getting sensitive data from point A to point B was cumbersome, given that private sector cloud providers were not permitted and data sizes quickly exceed email attachment limits. We heard reports from several agencies that sensitive data is often transferred by mailing hard drives or CDs from one department or agency to another, a technique that is slow and labor intensive. Several agencies also need to synchronize high volumes of data (multiple terabytes) on a regular basis between non-cloud data centers, which is starting to tax their infrastructure. The Patent Office, for example, is bound by law to store some parts of their approval process in private data centers, but when it moves on to other stages it can be pushed to the public cloud. This could mean up to 40 TB of data would need to be synchronized every 15 minutes in a worst-case scenario. This volume of data transfer is a technical challenge and is made more difficult by restrictions on use of the public cloud.

Others spoke about the data sharing problem from the discovery and inventory perspective. Participants strongly felt their agency had valuable data, but they couldn’t access it without a clear, well maintained directory. "We don’t know what we have," and “I know we have a lot of data, but it’s locked up in Excel spreadsheets across the agency” were some sentiments we heard several times. Indeed, Excel was a repeated source of frustration among the data professionals we spoke with, since both the data and the functions of the spreadsheets (for example, models which use the data) are difficult to take inventory of, share, and reuse at the organizational level. “My dream is I’d love to put Excel out of business” confessed one participant. The discovery problem was cited at both the inter- and intra-agency level: several participants longed to create richer datasets from data from other agencies, but were forced to speculate exactly what data the other agencies had available. 

Data integrity was also cited as a major stumbling block for agencies. We heard from many agencies who cited a lack of data cleanliness and accuracy as major hurdles to delivering more value from it. Most specifically cited sloppy data ingest as the source of diminished data accuracy. Widespread use of PDFs as a way to ingest data was a clear symptom of ingest problems that lead to high costs, poor data integrity, or both. Although technically "electronic", the format is nearly impossible to reliably and automatically convert into a structured format suitable for search and analysis. Several agencies bend over backwards to accept “any” input format, including images and paper, for what is essentially structured data, a policy that incurs heavy costs to preprocess the data into a structured format. Gathering data from states was frequently cited as a source of varied data formats and quality. It’s also important to note that data integrity issues due to poor ingest mechanisms may pose a high risk for agencies; one we spoke with estimated their target accuracy to convert an image format into a machine readable one was “99.999%”, since any error could have costly legal consequences. One participant astutely noted that the data integrity issue is closely tied to the data sharing issue; many times people don’t want to share their data because they’re not confident in its accuracy. 

Lack of staff or skills was also frequently cited as a pain point. In some cases, agencies had immediate needs that were not being met, at both the leadership and technical level. In other cases, agencies just felt they could be doing much more to further their mission if they had data experts. "We lack the capacity to dream," remarked one leader when asked what their data dreams were. They went on to explain a lack of cultural awareness of what is possible with data now. Several participants cited a relative abundance of analysts or data scientists, but a shortage of database administrators or IT staff able to supporting the toolset analysts need. Participants cited difficulty throughout the entire hiring pipeline: finding the budget, attracting talented data workers and leaders to apply, getting the resumes to their desk without being filtered by agency HR, and getting them to stay committed through a lengthy hiring process were all cited as contributing to a lack of data professionals. 

Draconian IT policies came up several times in our interviews as well. At many agencies, IT policies written for consuming, not creating software require each new download to go through a many-month review process. Yet, software development requires rapid exploration of the space of available software: developers may need to install 10 new libraries a day to explore solutions to a particular problem. The review process is just not practical in that environment, nor is it necessary. In the early stages, developers don’t need to consume or access any sensitive information or systems. Instead, what they claimed to need is a so-called "sandbox": a virtual, isolated environment where they can download software and interact with the environment from  their government hardware. We heard several reports of people paying out of pocket for private sector solutions to this problem since they were so critical. Even if analysts do go through the arduous process of getting approval, by the time the software is approved, the next version has come out and the analysts are forced to work with old tools. In a few cases, participants reported broken relationships with IT, where the IT team demonstrated little interest in supporting the tools and operating systems the analysts needed to do their jobs well. It’s clear that creative technical work in government requires revisiting some of these policies, offering secure workarounds, or both. One participant in this situation had a simple dream: “just to be able to get my job done.” 

Despite these pain points, participants expressed a lot of hope for the future and enthusiasm to using data to further their agency’s mission. When asked about their dreams, several participants were already actively engaged in contracts or internal work to make their dreams come true, and several had already undergone major transformations to streamline their data systems and reported a well-deserved pride in the results.

Conclusion

Our research has revealed clear themes in the data challenges individuals face across government: data sharing being the primary challenge, followed by data integrity, staffing, and IT policy. 

This is an ongoing effort. For thoughts, questions, and concerns, please reach out to [anthony.garvan@gsa.gov](mailto:anthony.garvan@gsa.gov). 

